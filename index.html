<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Yuanshen Guan</title>
  <meta name="author" content="Yuanshen Guan">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p class="name" style="text-align:center;">Yuanshen Guan</p>
                  <p>
                    I am a Ph.D. candidate in Information and Communication Engineering at the University of Science and
                    Technology of China, advised by Prof. Zhiwei Xiong. My work centers on extending generative models to
                    capture high dynamic range content, express arbitrary spatial attributes, and couple seamlessly with
                    efficient low-level vision systems.
                  </p>
                  <p>
                    I develop gain-map decomposed diffusion for text-to-HDR generation, spherical neural field diffusion
                    for viewpoint-controllable panoramas, and lightweight gain-map LUTs and fusion networks that turn
                    these generative priors into deployable HDR imaging pipelines across images and video.
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:guanys@mail.ustc.edu.cn">Email</a> &nbsp;/&nbsp;
                    <a href="resume_ysguan.pdf">Resume</a> &nbsp;/&nbsp;
                    <a href="https://github.com/Guanys-dar">GitHub</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:37%;max-width:37%;">
                  <a href="images/me.JPG"><img style="width:100%;max-width:100%;object-fit:cover;border-radius:50%;"
                      alt="profile photo placeholder" src="images/me.JPG"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:16px;width:100%;vertical-align:middle">
                  <h2>Education</h2>
                  <p>
                    <strong>University of Science and Technology of China</strong>, Hefei, Anhui<br>
                    Ph.D. Candidate in Information and Communication Engineering (Advisor: Prof. Zhiwei Xiong)<br>
                    National Scholarship; SZSE Scholarship<br>
                    2022 - 2027 (expected)
                  </p>
                  <p>
                    <strong>Northeastern University</strong>, Shenyang, Liaoning<br>
                    B.Eng. in Automation <br>
                    National Scholarship; First-Class Academic Scholarship<br>
                    2018 - 2022
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:16px;width:100%;vertical-align:middle">
                  <h2>Publications</h2>
                </td>
              </tr>
              <tr bgcolor="#ffffd0">
                <td style="padding:16px;width:100%;vertical-align:middle">
                  <div class="publication">
                    <img class="pub-thumb" src="images/thumbnails/iccv25_gmdiff.png"
                      alt="Thumbnail for HDR Image Generation via Gain Map Decomposed Diffusion">
                    <div class="pub-text">
                      <span class="papertitle">[ICCV 2025] HDR Image Generation via Gain Map Decomposed Diffusion</span>
                      <br>
                      <strong>Y. Guan</strong>, R. Xu, Y. Liao, M. Yao, L. Wang, Z. Xiong
                      <br>
                      <em>ICCV</em>, 2025
                      <p>
                        We introduce a gain-map decomposed diffusion pipeline that delivers the first text-to-HDR image
                        generator, stays compatible with ControlNet and mainstream Stable Diffusion extensions, and
                        scales SDR-to-HDR synthesis to native 4K resolution.
                      </p>
                    </div>
                  </div>
                </td>
              </tr>
              <tr>
                <td style="padding:16px;width:100%;vertical-align:middle">
                  <div class="publication">
                    <img class="pub-thumb" src="images/thumbnails/iclr26_arbpano.png"
                      alt="Thumbnail for Arbitrary-Shaped Image Generation via Spherical Neural Field Diffusion">
                    <div class="pub-text">
                      <span class="papertitle">[ICLR 2026] Arbitrary-Shaped Image Generation via Spherical Neural Field
                        Diffusion</span>
                      <br>
                      J. Xia*, <strong>Y. Guan*</strong>, R. Xu, J. Li, Z. Xiong
                      <br>
                      <em>ICLR</em>, 2026 (under review)
                      <p>
                        We perform diffusion over spherical neural fields with latent sampling, enabling explicit
                        control
                        over viewpoint, field of view, and output resolution to generate perspective, panoramic, and
                        fisheye imagery with sharply improved texture detail and geometric consistency.
                      </p>
                    </div>
                  </div>
                </td>
              </tr>
              <tr>
                <td style="padding:16px;width:100%;vertical-align:middle">
                  <div class="publication">
                    <img class="pub-thumb" src="images/thumbnails/ijcv_hdrdiff.png"
                      alt="Thumbnail for Diffusion-Promoted HDR Video Reconstruction">
                    <div class="pub-text">
                      <span class="papertitle">[IJCV] Diffusion-Promoted HDR Video Reconstruction</span>
                      <br>
                      <strong>Y. Guan</strong>, R. Xu, M. Yao, R. Gao, L. Wang, Z. Xiong
                      <br>
                      <em>International Journal of Computer Vision</em> (under review)
                      <p>
                        We investigate diffusion priors for HDR video reconstruction, treating noisy or motion-blurred
                        frames as out-of-distribution samples for targeted refinement, and achieve up to 15% LPIPS
                        improvements with cleaner temporal consistency.
                      </p>
                    </div>
                  </div>
                </td>
              </tr>
              <tr>
                <td style="padding:16px;width:100%;vertical-align:middle">
                  <div class="publication">
                    <img class="pub-thumb" src="images/thumbnails/iclr26_GMLUT.png"
                      alt="Thumbnail for Ultra-Fast Inverse Tone Mapping via Gain Map-based LUT">
                    <div class="pub-text">
                      <span class="papertitle">[ICLR 2026] Ultra-Fast Inverse Tone Mapping via Gain Map-based LUT</span>
                      <br>
                      <strong>Y. Guan</strong>, R. Xu, Y. Liao, Z. Xiong
                      <br>
                      <em>ICLR</em>, 2026 (under review)
                      <p>
                        We design a gain-map based lookup table architecture and curate a paired SDR-to-gain-map
                        dataset,
                        enabling high-fidelity SDR-to-HDR conversion in 6.2 ms per 4K frame on a V100 GPU with a 1.4 dB
                        PSNR improvement over prior inverse tone mapping methods.
                      </p>
                    </div>
                  </div>
                </td>
              </tr>
              <tr>
                <td style="padding:16px;width:100%;vertical-align:middle">
                  <div class="publication">
                    <img class="pub-thumb" src="images/thumbnails/iclr25_gmnet.png"
                      alt="Thumbnail for Learning Gain Map for Inverse Tone Mapping">
                    <div class="pub-text">
                      <span class="papertitle">[ICLR 2025] Learning Gain Map for Inverse Tone Mapping</span>
                      <br>
                      Y. Liao, <strong>Y. Guan</strong>, R. Xu, J. Li, S. Sun, Z. Xiong
                      <br>
                      <em>ICLR</em>, 2025
                      <p>
                        We present the first inverse tone mapping model that directly learns gain maps, together with
                        synthetic and real benchmark datasets, demonstrating gains of up to 2.7 dB PSNR over previous
                        inverse tone mapping networks.
                      </p>
                    </div>
                  </div>
                </td>
              </tr>
              <tr>
                <td style="padding:16px;width:100%;vertical-align:middle">
                  <div class="publication">
                    <img class="pub-thumb" src="images/thumbnails/acmmm23_mgdn.png"
                      alt="Thumbnail for Mutual-Guided Dynamic Network for Image Fusion">
                    <div class="pub-text">
                      <span class="papertitle">[ACMMM 2024] Mutual-Guided Dynamic Network for Image Fusion</span>
                      <br>
                      <strong>Y. Guan</strong>, R. Xu, M. Yao, L. Wang, Z. Xiong
                      <br>
                      <em>ACM Multimedia</em>, 2024
                      <p>
                        We couple dynamic convolution with a mutual-information alignment loss to build a unified image
                        fusion framework, reaching state-of-the-art across seven benchmarks spanning multi-focus,
                        multi-exposure, RGB-infrared, RGB-depth, and HDR de-ghosting tasks.
                      </p>
                    </div>
                  </div>
                </td>
              </tr>
              <tr>
                <td style="padding:16px;width:100%;vertical-align:middle">
                  <div class="publication">
                    <img class="pub-thumb" src="images/thumbnails/tomm_editor.png"
                      alt="Thumbnail for EdiTor: Edge-guided Transformer for Ghost-free HDR Imaging">
                    <div class="pub-text">
                      <span class="papertitle">[ACM TOMM] EdiTor: Edge-guided Transformer for Ghost-free HDR
                        Imaging</span>
                      <br>
                      <strong>Y. Guan</strong>, R. Xu, M. Yao, J. Huang, Z. Xiong
                      <br>
                      <em>ACM Transactions on Multimedia Computing, Communications, and Applications</em>
                      <p>
                        We use image gradients as illumination-invariant cues to guide transformer self-attention,
                        making
                        cross-exposure correspondence robust and ghost-free, and deliver state-of-the-art quantitative
                        and perceptual HDR results on three public benchmarks.
                      </p>
                    </div>
                  </div>
                </td>
              </tr>
              <tr>
                <td style="padding:16px;width:100%;vertical-align:middle">
                  <div class="publication">
                    <img class="pub-thumb" src="images/thumbnails/tci_arbsr.png"
                      alt="Thumbnail for Learning Continuous Degradation for Arbitrary-Scale Blind Super-Resolution">
                    <div class="pub-text">
                      <span class="papertitle">[IEEE TCI] Learning Continuous Degradation for Arbitrary-Scale Blind
                        Super-Resolution</span>
                      <br>
                      J. Xia, <strong>Y. Guan</strong>, R. Xu, J. Li, M. Yao, Z. Xiong
                      <br>
                      <em>IEEE Transactions on Computational Imaging</em>
                      <p>
                        We model degradation as a continuous latent trajectory, allowing a single network to handle
                        arbitrary upscaling factors while restoring sharp details under unknown blur and noise.
                      </p>
                    </div>
                  </div>
                </td>
              </tr>
              <tr>
                <td style="padding:16px;width:100%;vertical-align:middle">
                  <div class="publication">
                    <img class="pub-thumb" src="images/thumbnails/tci_ruikang.png"
                      alt="Thumbnail for Learning Piece-wise Planar Representation for Guided Depth Super-Resolution">
                    <div class="pub-text">
                      <span class="papertitle">[IEEE TCI] Learning Piece-wise Planar Representation for Guided Depth
                        Super-Resolution</span>
                      <br>
                      R. Xu, M. Yao, <strong>Y. Guan</strong>, Z. Xiong
                      <br>
                      <em>IEEE Transactions on Computational Imaging</em>
                      <p>
                        We learn piece-wise planar representations to guide depth super-resolution, preserving geometric
                        boundaries and suppressing texture-copy artifacts in guided depth completion.
                      </p>
                    </div>
                  </div>
                </td>
              </tr>
              <tr>
                <td style="padding:16px;width:100%;vertical-align:middle">
                  <div class="publication">
                    <img class="pub-thumb" src="images/thumbnails/tip_allinone.png"
                      alt="Thumbnail for Neural Degradation Representation Learning for All-In-One Image Restoration">
                    <div class="pub-text">
                      <span class="papertitle">[IEEE TIP] Neural Degradation Representation Learning for All-In-One
                        Image
                        Restoration</span>
                      <br>
                      M. Yao, R. Xu, <strong>Y. Guan</strong>, J. Huang, Z. Xiong
                      <br>
                      <em>IEEE Transactions on Image Processing</em>
                      <p>
                        We train a shared degradation-aware prior for all-in-one image restoration, enabling a single
                        model to adapt to diverse noise, blur, and compression scenarios with strong quantitative and
                        visual quality.
                      </p>
                    </div>
                  </div>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:16px;width:100%;vertical-align:middle">
                  <h2>Honors &amp; Awards</h2>
                  <ul>
                    <li>National Scholarship (Graduate) - 2024</li>
                    <li>National Scholarship (Undergraduate) - 2020</li>
                    <li>First-Class Scholarships - 2019, 2021, 2022, 2023, 2024</li>
                    <li>IELTS 7.0, GRE 325 (Math:170, Verbal:155)</li>
                    <li>National Bronze Award, China "Internet+" Innovation Competition - 2021</li>
                    <li>National First Prize, China Collegiate Computer Design Competition - 2021</li>
                    <li>UG2+ Challenge, 4th Place (Atmospheric Turbulence) - 2023</li>
                    <li>MIPI Challenge, 4th Place (RGBW Fusion and Denoising) - 2022</li>
                    <li>Honorable Mention, Mathematical Contest in Modeling (MCM) - 2021</li>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
    </tbody>
  </table>
</body>

</html>
